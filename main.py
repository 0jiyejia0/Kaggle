import pandas as pd
import numpy as np
from data_preprocessing import load_and_preprocess_data
from feature_engineering import add_features, select_features
from model_training import train_model_cv
from ensemble import calculate_weights, blend_predictions, stacking_predictions
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æŠ¥å‘Šç”Ÿæˆå™¨çš„ä¸»å‡½æ•°
from experiment_report_generator import main as generate_report # æˆ–è€…å¯¼å…¥ create_performance_report
from eda_analysis import generate_eda_plots

def setup_experiment_tracking():
    """è®¾ç½®å®éªŒè·Ÿè¸ª"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_dir = f"experiments/{timestamp}"
    os.makedirs(experiment_dir, exist_ok=True)
    os.makedirs(f"{experiment_dir}/models", exist_ok=True)
    os.makedirs(f"{experiment_dir}/plots", exist_ok=True)
    os.makedirs(f"{experiment_dir}/predictions", exist_ok=True)
    return experiment_dir

def save_experiment_config(config, experiment_dir):
    """ä¿å­˜å®éªŒé…ç½®"""
    with open(f"{experiment_dir}/config.json", "w") as f:
        json.dump(config, f, indent=4)

def plot_feature_importance(feature_importance, experiment_dir):
    """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾"""
    try:
        plt.figure(figsize=(12, 6))
        importance_df = pd.DataFrame(feature_importance.items(), columns=['Feature', 'Importance'])
        importance_df = importance_df.sort_values('Importance', ascending=False)
        
        sns.barplot(data=importance_df.head(20), x='Importance', y='Feature')
        plt.title('Top 20 Most Important Features')
        plt.tight_layout()
        plt.savefig(f"{experiment_dir}/plots/feature_importance.png")
        plt.close()
        print("âœ… ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜")
    except Exception as e:
        print(f"âŒ ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾æ—¶å‡ºé”™: {str(e)}")

def safe_model_training(model_name, X_train, y_train_log, y_train, X_test, experiment_dir, n_splits=3):
    """å®‰å…¨çš„æ¨¡å‹è®­ç»ƒï¼Œå¸¦æœ‰å®Œæ•´é”™è¯¯å¤„ç†"""
    try:
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {model_name} æ¨¡å‹...")
        model_output_dir = f"{experiment_dir}/models"
        result = train_model_cv(
            model_name=model_name,
            X=X_train,
            y_log=y_train_log,
            y=y_train,
            X_test=X_test,
            output_model_dir=model_output_dir,
            n_splits=n_splits
        )
        
        # è¯¦ç»†æ£€æŸ¥è¿”å›ç»“æœ
        if result is None:
            print(f"âŒ {model_name} è®­ç»ƒè¿”å› None")
            return None
            
        if not isinstance(result, dict):
            print(f"âŒ {model_name} è®­ç»ƒè¿”å›éå­—å…¸ç±»å‹: {type(result)}")
            return None
            
        # æ£€æŸ¥å¿…è¦çš„é”®
        required_keys = ["final_test_pred", "metrics"]
        missing_keys = [key for key in required_keys if key not in result]
        if missing_keys:
            print(f"âŒ {model_name} è®­ç»ƒç»“æœç¼ºå°‘é”®: {missing_keys}")
            return None
            
        # æ£€æŸ¥é¢„æµ‹ç»“æœ
        if result.get("final_test_pred") is None:
            print(f"âŒ {model_name} é¢„æµ‹ç»“æœä¸ºç©º")
            return None
            
        if result.get("metrics") is not None:
            metrics = result["metrics"]
            print(f"âœ… {model_name} è®­ç»ƒæˆåŠŸ! RMSE: {metrics.get('rmse', 'N/A'):.4f}, RÂ²: {metrics.get('r2', 'N/A'):.4f}")
            return result
        else:
            print(f"âš ï¸ {model_name} è®­ç»ƒå®Œæˆä½†ç¼ºå°‘è¯„ä¼°æŒ‡æ ‡")
            return result
            
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ {model_name} è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        return None
    except Exception as e:
        print(f"âŒ {model_name} è®­ç»ƒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def main():
    print("ğŸ¯ === å¼€å§‹æœ€ç»ˆç‰ˆæœ¬çš„æ¨¡å‹è®­ç»ƒ ===")
    
    # é¦–å…ˆï¼Œæ‰§è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆå¦‚æœå›¾è¡¨ä¸å­˜åœ¨ï¼‰
    generate_eda_plots()
    
    # è®¾ç½®å®éªŒè·Ÿè¸ª
    experiment_dir = setup_experiment_tracking()
    
    # ä¼˜åŒ–çš„é…ç½®å‚æ•°
    config = {
        "random_seed": 42,
        "cv_folds": 5,  # <--- ä¿®æ”¹ä¸º5æŠ˜
        "use_gpu": False,  # ä½¿ç”¨CPUç¡®ä¿ç¨³å®šæ€§
        "feature_selection": {
            "n_features": 30,  # å‡å°‘ç‰¹å¾æ•°é‡
            "method": "selectkbest"
        },
        "models": {
            "xgb": {
                "enabled": True,
                "params": {
                    "n_estimators": 1000,
                    "learning_rate": 0.05,
                    "max_depth": 6
                }
            },
            "lgb": {
                "enabled": True,
                "params": {
                    "n_estimators": 1000,
                    "learning_rate": 0.05,
                    "num_leaves": 31,
                    "min_child_samples": 10,
                    "min_split_gain": 0.01,
                    "reg_alpha": 0.01,
                    "reg_lambda": 0.01
                }
            },
            "cat": {
                "enabled": True,
                "params": {
                    "iterations": 500,
                    "learning_rate": 0.05,
                    "depth": 6,
                    "task_type": "CPU",
                    "bootstrap_type": "Bernoulli",
                    "subsample": 0.8
                }
            },
            "tabnet": {
                "enabled": True,  # <--- å¯ç”¨TabNet
                "params": {
                    "n_d": 24,
                    "n_a": 24,
                    "n_steps": 5
                }
            }
        },
        "ensemble": {
            "methods": ["weighted", "stacking"],
            "weights_method": "auto"
        }
    }
    
    # ä¿å­˜é…ç½®
    save_experiment_config(config, experiment_dir)
    
    print("\nğŸ“Š === å¼€å§‹æ•°æ®å¤„ç† ===")
    try:
        # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        X_train, X_test, y_train, y_test, test_ids = load_and_preprocess_data()
        print(f"ğŸ“ˆ åˆå§‹æ•°æ®å½¢çŠ¶: X_train={X_train.shape}, X_test={X_test.shape}")
        
        # ç‰¹å¾å·¥ç¨‹ï¼ˆä½¿ç”¨ä¿®å¤ç‰ˆæœ¬ï¼‰
        print("\nğŸ”§ === å¼€å§‹ç‰¹å¾å·¥ç¨‹ ===")
        X_train, X_test = add_features(X_train, X_test)
        
        # ç‰¹å¾é€‰æ‹©
        if config["feature_selection"]["method"] == "selectkbest":
            print("ğŸ¯ å¼€å§‹ç‰¹å¾é€‰æ‹©...")
            selected_features = select_features(X_train, y_train, k=config["feature_selection"]["n_features"])
            print(f"âœ… é€‰æ‹©äº† {len(selected_features)} ä¸ªç‰¹å¾")
            
            # ç¡®ä¿é€‰æ‹©çš„ç‰¹å¾åœ¨æ•°æ®ä¸­å­˜åœ¨
            available_features = [f for f in selected_features if f in X_train.columns]
            X_train = X_train[available_features]
            X_test = X_test[available_features]
            print(f"ğŸ“‹ æœ€ç»ˆä½¿ç”¨ {len(available_features)} ä¸ªç‰¹å¾")
        
        # å¯¹ç›®æ ‡å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢
        y_train_log = np.log1p(y_train)
        
        print(f"\nğŸ“Š å¤„ç†åæ•°æ®å½¢çŠ¶: X_train={X_train.shape}, X_test={X_test.shape}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return
    
    # è®­ç»ƒå„ä¸ªæ¨¡å‹
    print("\nğŸ¤– === å¼€å§‹æ¨¡å‹è®­ç»ƒ ===")
    models_results = {}
    test_predictions = {}
    model_metrics = {}
    oof_predictions = {}
    oof_predictions_log = {}
    test_predictions_log = {}
    
    enabled_models = [name for name, model_config in config["models"].items() if model_config["enabled"]]
    print(f"ğŸ¯ å°†è®­ç»ƒä»¥ä¸‹æ¨¡å‹: {enabled_models}")
    
    for i, model_name in enumerate(enabled_models, 1):
        print(f"\nğŸ”„ è¿›åº¦: {i}/{len(enabled_models)} - è®­ç»ƒ {model_name}")
        result = safe_model_training(model_name, X_train, y_train_log, y_train, X_test, experiment_dir, config["cv_folds"])
        
        if result is not None:
            models_results[model_name] = result
            
            # å­˜å‚¨æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
            if "final_test_pred" in result and result["final_test_pred"] is not None:
                test_predictions[model_name] = result["final_test_pred"]
                print(f"âœ… {model_name} é¢„æµ‹ç»“æœå·²ä¿å­˜")

            # å­˜å‚¨OOFé¢„æµ‹ç”¨äºStacking
            if "oof_preds" in result and result["oof_preds"] is not None:
                oof_predictions[model_name] = result["oof_preds"]
                print(f"âœ… {model_name} OOFé¢„æµ‹å·²ä¿å­˜")
            
            # å­˜å‚¨å¯¹æ•°å°ºåº¦çš„OOFå’Œæµ‹è¯•é›†é¢„æµ‹ï¼Œç”¨äºStacking
            if "oof_preds_log" in result and result["oof_preds_log"] is not None:
                oof_predictions_log[model_name] = result["oof_preds_log"]
            if "test_preds_cv_log" in result and result["test_preds_cv_log"] is not None:
                test_predictions_log[model_name] = result["test_preds_cv_log"]

            # å­˜å‚¨æ¯ä¸ªæ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡
            if "metrics" in result and result["metrics"] is not None:
                model_metrics[model_name] = result["metrics"]
                print(f"âœ… {model_name} è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜")
        else:
            print(f"âŒ è·³è¿‡å¤±è´¥çš„æ¨¡å‹: {model_name}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸè®­ç»ƒçš„æ¨¡å‹
    if len(test_predictions) == 0:
        print("âŒ æ²¡æœ‰æ¨¡å‹æˆåŠŸè®­ç»ƒï¼Œç¨‹åºç»“æŸ")
        return
    
    print(f"\nğŸ‰ æˆåŠŸè®­ç»ƒçš„æ¨¡å‹: {list(test_predictions.keys())} (å…± {len(test_predictions)} ä¸ª)")
    
    # æ˜¾ç¤ºå„æ¨¡å‹æ€§èƒ½
    print("\nğŸ“Š === æ¨¡å‹æ€§èƒ½æ±‡æ€» ===")
    for model_name, metrics in model_metrics.items():
        if metrics:
            print(f"ğŸ”¸ {model_name}: RMSE={metrics.get('rmse', 'N/A'):.0f}, RÂ²={metrics.get('r2', 'N/A'):.4f}")
    
    # æ¨¡å‹èåˆ
    print("\nğŸ”€ === å¼€å§‹æ¨¡å‹èåˆ ===")
    
    # æ£€æŸ¥å¯ç”¨çš„èåˆæ–¹æ³•
    ensemble_methods = config.get("ensemble", {}).get("methods", [])

    # --- 1. åŠ æƒå¹³å‡èåˆ ---
    if "weighted" in ensemble_methods:
        try:
            print("\n--- å¼€å§‹åŠ æƒå¹³å‡èåˆ ---")
            if len(model_metrics) > 0:
                # è·å–å„ä¸ªæ¨¡å‹çš„æƒé‡
                weights = calculate_weights(model_metrics)
                print("\nâš–ï¸ æ¨¡å‹æƒé‡:")
                for model_name, weight in weights.items():
                    print(f"   ğŸ”¸ {model_name}: {weight:.4f}")
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡æ ‡ï¼Œä½¿ç”¨ç­‰æƒé‡
                weights = {name: 1.0/len(test_predictions) for name in test_predictions.keys()}
                print("âš–ï¸ ä½¿ç”¨ç­‰æƒé‡èåˆ")
            
            # åŠ æƒå¹³å‡èåˆ
            if len(test_predictions) > 1:
                ensemble_pred = blend_predictions(
                    list(test_predictions.values()),
                    list(weights.values()),
                    method="weighted"
                )
                print("âœ… å¤šæ¨¡å‹åŠ æƒèåˆå®Œæˆ")
            else:
                # å¦‚æœåªæœ‰ä¸€ä¸ªæ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨å…¶é¢„æµ‹
                ensemble_pred = list(test_predictions.values())[0]
                print("âœ… å•æ¨¡å‹é¢„æµ‹")
            
            # ä¿å­˜é¢„æµ‹ç»“æœ
            if ensemble_pred is not None:
                submission_df = pd.DataFrame({
                    'Id': test_ids,
                    'SalePrice': ensemble_pred
                })
                submission_path = f"{experiment_dir}/predictions/submission_weighted.csv"
                submission_df.to_csv(submission_path, index=False)
                print(f"âœ… åŠ æƒå¹³å‡é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {submission_path}")
            
        except Exception as e:
            print(f"âŒ åŠ æƒå¹³å‡èåˆå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()

    # --- 2. Stacking èåˆ ---
    if "stacking" in ensemble_methods:
        try:
            print("\n--- å¼€å§‹Stackingèåˆ ---")
            if len(oof_predictions) < 2 or len(test_predictions) < 2:
                print("âš ï¸ Stackingéœ€è¦è‡³å°‘2ä¸ªæ¨¡å‹ï¼Œè·³è¿‡æ­¤æ­¥éª¤ã€‚")
            else:
                # ç¡®ä¿OOFå’Œæµ‹è¯•é¢„æµ‹çš„æ¨¡å‹ä¸€è‡´
                common_models = sorted(list(set(oof_predictions_log.keys()) & set(test_predictions_log.keys())))
                
                if len(common_models) < 2:
                    print("âš ï¸ Stackingæ‰€éœ€æ¨¡å‹çš„å¯¹æ•°å°ºåº¦é¢„æµ‹ä¸å®Œæ•´ï¼Œè·³è¿‡æ­¤æ­¥éª¤ã€‚")
                else:
                    oof_preds_aligned = {model: oof_predictions_log[model] for model in common_models}
                    test_preds_aligned = {model: test_predictions_log[model] for model in common_models}

                    print(f"ç”¨äºStackingçš„æ¨¡å‹: {common_models}")

                    stacking_pred = stacking_predictions(
                        oof_predictions_log=oof_preds_aligned,
                        test_predictions_log=test_preds_aligned,
                        y_train_log=y_train_log,
                        model_dir=f"{experiment_dir}/models"
                    )
                
                    if stacking_pred is not None:
                        submission_df = pd.DataFrame({
                            'Id': test_ids,
                            'SalePrice': stacking_pred
                        })
                        submission_path = f"{experiment_dir}/predictions/submission_stacking.csv"
                        submission_df.to_csv(submission_path, index=False)
                        print(f"âœ… Stackingé¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {submission_path}")

        except Exception as e:
            print(f"âŒ Stackingèåˆå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()

    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    try:
        if "xgb" in models_results and models_results["xgb"]["final_model"] is not None:
            feature_importance = dict(zip(
                X_train.columns,
                models_results["xgb"]["final_model"].feature_importances_
            ))
            plot_feature_importance(feature_importance, experiment_dir)
    except Exception as e:
        print(f"âŒ ç‰¹å¾é‡è¦æ€§åˆ†æå¤±è´¥: {str(e)}")
    
    print(f"\nğŸ‰ === å®éªŒå®Œæˆ ===")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {experiment_dir}")
    print(f"ğŸ† æˆåŠŸè®­ç»ƒçš„æ¨¡å‹æ€»æ•°: {len(models_results)}")
    
    # ä¿å­˜ä¸ªåˆ«æ¨¡å‹é¢„æµ‹ï¼ˆç”¨äºåˆ†æï¼‰
    try:
        for model_name, pred in test_predictions.items():
            individual_path = f"{experiment_dir}/predictions/predictions_{model_name}.csv"
            pd.DataFrame({
                'Id': range(1, len(pred) + 1),
                'SalePrice': pred
            }).to_csv(individual_path, index=False)
            print(f"ğŸ“„ {model_name} å•ç‹¬é¢„æµ‹å·²ä¿å­˜")
    except Exception as e:
        print(f"âŒ ä¿å­˜å•ç‹¬é¢„æµ‹æ—¶å‡ºé”™: {str(e)}")
    
    # æœ€ç»ˆæ€»ç»“
    print(f"\nğŸ === è®­ç»ƒæ€»ç»“ ===")
    print(f"âœ… æˆåŠŸæ¨¡å‹æ•°: {len(test_predictions)}")
    print(f"ğŸ“Š ç›®æ ‡æ¨¡å‹æ•°: {len(enabled_models)}")
    if len(test_predictions) == len(enabled_models):
        print("ğŸ‰ æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹éƒ½è®­ç»ƒæˆåŠŸï¼")
    else:
        failed_models = set(enabled_models) - set(test_predictions.keys())
        print(f"âš ï¸ å¤±è´¥çš„æ¨¡å‹: {failed_models}")

    # åœ¨å®éªŒç»“æŸåç”ŸæˆæŠ¥å‘Š
    print(f"\nğŸ“‹ === å¼€å§‹ç”Ÿæˆå®éªŒæŠ¥å‘Š ===")
    try:
        success = generate_report(experiment_dir) # è°ƒç”¨æŠ¥å‘Šç”Ÿæˆå™¨çš„ä¸»å‡½æ•°
        if success:
            print("âœ… å®éªŒæŠ¥å‘Šå·²ç”Ÿæˆ")
        else:
            print("âš ï¸ å®éªŒæŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ä¸Šæ–¹æ—¥å¿—ã€‚")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå®éªŒæŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 